{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-17T06:44:56.907108Z","iopub.execute_input":"2022-04-17T06:44:56.907431Z","iopub.status.idle":"2022-04-17T06:44:56.918704Z","shell.execute_reply.started":"2022-04-17T06:44:56.907391Z","shell.execute_reply":"2022-04-17T06:44:56.917952Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport torch\nfrom tqdm import tqdm\nfrom transformers import BertPreTrainedModel, BertModel\nfrom transformers.models.bert.modeling_bert import BertOnlyMLMHead\nfrom transformers import get_linear_schedule_with_warmup, AdamW\nfrom torch import nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:44:57.718019Z","iopub.execute_input":"2022-04-17T06:44:57.718288Z","iopub.status.idle":"2022-04-17T06:44:57.725284Z","shell.execute_reply.started":"2022-04-17T06:44:57.718261Z","shell.execute_reply":"2022-04-17T06:44:57.724325Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"from pydantic import BaseModel\nfrom typing import Type\n\nclass GolbalOption(BaseModel):\n    # 本地模型存盘目录\n    local_model_dir = 'local_model'\n    # 数据集所在目录\n    dataset_dir = '/kaggle/input/agnews/'\n    # 类别标签文件\n    label_names_file = 'label_names.txt'\n    # 训练文件和转换后存盘文件\n    train_file = 'train.txt'\n    train_load_file = 'train.pt'\n    # 测试文件和转换后存盘文件\n    test_file = 'test.txt'\n    test_label_file = 'test_labels.txt'\n    test_load_file = 'test.pt'\n    # 构建分类词汇表用数据存盘文件\n    label_name_load_file = 'label_name_data.pt'\n    # 分类词汇表存盘文件\n    category_vocab_load_file = 'category_vocab.pt'\n    # mcp任务存盘文件\n    mcp_train_load_file = 'train_data.pt'\n    mcp_load_file = 'mcp_model.pt'\n    # 模型存盘文件\n    final_model = 'final_model.pt'\n    out_file = 'out.txt'\n    # 模型训练参数\n    eval_batch_size = 128\n    train_batch_size = 32\n    top_pred_num = 50\n    category_vocab_size = 100\n    match_threshold = 20\n    max_len = 200\n    update_interval = 50\n    accum_steps = 4\n    mcp_epochs = 5\n    self_train_epochs = 1\n    early_stop = True\n    \n    bert_model = 'bert-base-uncased'\n    \n    # 扩展参数配置（实例化后赋值）\n    device: str = None\n    tokenizer : Type\n    vocab: dict = None\n    vocab_size : int = None\n    mask_id: int = None\n    inv_vocab : dict = None\n    label_name_dict : dict = None\n    label2class: dict = None\n    num_class: int = None\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:44:57.727206Z","iopub.execute_input":"2022-04-17T06:44:57.728083Z","iopub.status.idle":"2022-04-17T06:44:57.742957Z","shell.execute_reply.started":"2022-04-17T06:44:57.727934Z","shell.execute_reply":"2022-04-17T06:44:57.742160Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def read_label_names(opt):\n    \"\"\"\n    从文件中读取标签名\n    \"\"\"\n    label_name_file = open(os.path.join(opt.dataset_dir, opt.label_names_file))\n    label_names = label_name_file.readlines()\n    # 读取每个标签中的单词list，以行号作为类别id存入字典 {0:[word1,word2,...], 1:[word1,word2,...], ...}\n    label_name_dict = {i: [word.lower() for word in category_words.strip().split()] for i, category_words in enumerate(label_names)}\n    print(f\"每个类别使用的标签名称分别是: {label_name_dict}\")\n    # 所有标签类别映射字典\n    label2class = {}\n    for class_idx in label_name_dict:\n        for word in label_name_dict[class_idx]:\n            # assert标记用作标签名称的单词\n            assert word not in label2class, f\"\\\"{word}\\\" 作为标签名，被应用在多分类任务中\"\n            # 类别->类别id\n            label2class[word] = class_idx\n    return label_name_dict, label2class","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:44:57.770571Z","iopub.execute_input":"2022-04-17T06:44:57.771081Z","iopub.status.idle":"2022-04-17T06:44:57.777666Z","shell.execute_reply.started":"2022-04-17T06:44:57.771053Z","shell.execute_reply":"2022-04-17T06:44:57.776890Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\nopt= GolbalOption(\n    tokenizer=BertTokenizerFast\n)\n# 训练设备\nopt.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# tokenizer\nopt.tokenizer = BertTokenizerFast.from_pretrained(opt.bert_model, model_max_length=opt.max_len)\n# 词汇表\nopt.vocab = opt.tokenizer.get_vocab()\n# 词汇表长度\nopt.vocab_size = len(opt.vocab)\n# [MASK]掩码id\nopt.mask_id = opt.vocab[opt.tokenizer.mask_token]\n# 反向词汇表\nopt.inv_vocab = {k:v for v, k in opt.vocab.items()}\n# label信息\nopt.label_name_dict, opt.label2class = read_label_names(opt)\n# 类别数量\nopt.num_class = len(opt.label_name_dict)\n\n# 检验并创建本地模型存盘目录\nif not os.path.exists(opt.local_model_dir):\n    os.makedirs(opt.local_model_dir)\n\n# print(opt.device)\n# print(opt.tokenizer('have fun!'))\n# print(len(opt.vocab))\n# print(opt.num_class)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:44:57.879468Z","iopub.execute_input":"2022-04-17T06:44:57.879758Z","iopub.status.idle":"2022-04-17T06:45:04.462130Z","shell.execute_reply.started":"2022-04-17T06:44:57.879730Z","shell.execute_reply":"2022-04-17T06:45:04.460379Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"#### LOTClass模型","metadata":{}},{"cell_type":"code","source":"class LOTClassModel(BertPreTrainedModel):\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        self.init_weights()\n        # MLM head is not trained\n        for param in self.cls.parameters():\n            param.requires_grad = False\n    \n    def forward(self, input_ids, pred_mode, attention_mask=None, token_type_ids=None, \n                position_ids=None, head_mask=None, inputs_embeds=None):\n        bert_outputs = self.bert(input_ids,\n                                 attention_mask=attention_mask,\n                                 token_type_ids=token_type_ids,\n                                 position_ids=position_ids,\n                                 head_mask=head_mask,\n                                 inputs_embeds=inputs_embeds)\n        last_hidden_states = bert_outputs[0]\n        if pred_mode == \"classification\":\n            trans_states = self.dense(last_hidden_states)\n            trans_states = self.activation(trans_states)\n            trans_states = self.dropout(trans_states)\n            logits = self.classifier(trans_states)\n        elif pred_mode == \"mlm\":\n            logits = self.cls(last_hidden_states)\n        else:\n            sys.exit(\"Wrong pred_mode!\")\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:45:04.464055Z","iopub.execute_input":"2022-04-17T06:45:04.464331Z","iopub.status.idle":"2022-04-17T06:45:04.475864Z","shell.execute_reply.started":"2022-04-17T06:45:04.464295Z","shell.execute_reply":"2022-04-17T06:45:04.475117Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"#### 创建分类词汇表","metadata":{}},{"cell_type":"code","source":"def create_input_dataset(opt, text_file, loader_name, label_file=None):\n    # 尝试加载train.pt存盘文件。该文件中存储的就是经tokenizer编码后的input_ids和attention_masks\n    loader_file = os.path.join(opt.local_model_dir, loader_name)\n    if os.path.exists(loader_file):\n        print(f\"从 {loader_file} 文件中加载编码后的模型输入张量\")\n        data = torch.load(loader_file)\n    else:\n        print(f\"从{os.path.join(opt.dataset_dir, text_file)}文件中读取语料\")\n        corpus = open(os.path.join(opt.dataset_dir, text_file), encoding=\"utf-8\")\n        docs = [doc.strip() for doc in corpus.readlines()]\n        print(f\"转换文本为tensor集合\")\n        encoded_dict = opt.tokenizer(docs, add_special_tokens=True, max_length=opt.max_len, padding='max_length',\n                                  return_attention_mask=True, truncation=True, return_tensors='pt')\n        input_ids = encoded_dict['input_ids']\n        attention_masks = encoded_dict['attention_mask']\n    \n        print(f\"编码后的文本语料存入文件:{loader_file}\")\n        if label_file is not None:\n            print(f\"从 {os.path.join(opt.dataset_dir, label_file)} 文件中读取Label\")\n            truth = open(os.path.join(opt.dataset_dir, label_file))\n            labels = [int(label.strip()) for label in truth.readlines()]\n            labels = torch.tensor(labels)\n            data = {\"input_ids\": input_ids, \"attention_masks\": attention_masks, \"labels\": labels}\n        else:\n            data = {\"input_ids\": input_ids, \"attention_masks\": attention_masks}\n        torch.save(data, loader_file)\n    return data\n\ndef create_label_name_dataset(opt, text_file, loader_name):\n    \"\"\"\n    根据提供的语料生成，用于创建“分类词汇表”的模型tensor\n    \"\"\"\n    # 尝试加载存盘文件\n    loader_file = os.path.join(opt.local_model_dir, loader_name)\n    if os.path.exists(loader_file):\n        print(f\"从 {loader_file} 文件中加载包含标签名的语料张量\")\n        label_name_data = torch.load(loader_file)\n    else:\n        print(f\"从 {os.path.join(opt.dataset_dir, text_file)} 文件中读取语料\")\n        corpus = open(os.path.join(opt.dataset_dir, text_file), encoding=\"utf-8\")\n        docs = [doc.strip() for doc in corpus.readlines()]\n        print(\"检索包含类别词汇的语料\")\n        input_ids_with_label_name, \\\n        attention_masks_with_label_name, \\\n        label_name_idx = label_name_occurrence(opt, docs)\n\n        assert len(input_ids_with_label_name) > 0, \"语料中没有发现匹配的标签名!\"\n        label_name_data = {\n            \"input_ids\": input_ids_with_label_name, \n            \"attention_masks\": attention_masks_with_label_name, \n            \"labels\": label_name_idx\n        }\n        # 数据存盘\n        print(f\"包含标签名的语料张量存入文件 {loader_file}\")\n        torch.save(label_name_data, loader_file)\n    return label_name_data\n\ndef label_name_occurrence(opt, docs):\n    \"\"\"\n    查找包含标签名的语料\n    \"\"\"\n    text_with_label = []\n    label_name_idx = []\n    for doc in tqdm(docs, desc='文档检索'):\n        result = label_name_in_doc(opt, doc)\n        if result is not None:\n            text_with_label.append(result[0])\n            label_name_idx.append(result[1].unsqueeze(0))\n    # 如果有符合条件的文本，就把文本转换为模型输入用的tensor并返回\n    # 如果没有，就返回一些值全为1的张量\n    if len(text_with_label) > 0:\n        encoded_dict = opt.tokenizer(text_with_label, add_special_tokens=True, max_length=opt.max_len, \n                                 padding='max_length', return_attention_mask=True, truncation=True, return_tensors='pt')\n        input_ids_with_label_name = encoded_dict['input_ids']\n        attention_masks_with_label_name = encoded_dict['attention_mask']\n        label_name_idx = torch.cat(label_name_idx, dim=0)\n    else:\n        input_ids_with_label_name = torch.ones(0, opt.max_len, dtype=torch.long)\n        attention_masks_with_label_name = torch.ones(0, opt.max_len, dtype=torch.long)\n        label_name_idx = torch.ones(0, opt.max_len, dtype=torch.long)\n    return input_ids_with_label_name, attention_masks_with_label_name, label_name_idx\n\ndef label_name_in_doc(opt, doc):\n    \"\"\"\n    在现有文本中查找标签名称并标记索引，用[MASK]替换超出词典范围的标签名称\n    \"\"\"\n    # 拆分文本\n    doc = opt.tokenizer.tokenize(doc)\n    # 创建一个和语料长度一致，全部值都是-1的张量序列作为label\n    label_idx = -1 * torch.ones(opt.max_len, dtype=torch.long)\n    new_doc = []\n    wordpcs = []\n    idx = 1 # 由于[CLS] token，所以索引从1开始\n    for i, wordpc in enumerate(doc):\n        # 添加词汇和子词到wordpcs\n        wordpcs.append(wordpc[2:] if wordpc.startswith(\"##\") else wordpc)\n        if idx >= opt.max_len - 1: # 最后一个索引应当是 [SEP]\n            break\n        if i == len(doc) - 1 or not doc[i+1].startswith(\"##\"):\n            word = ''.join(wordpcs)\n            # 如果词汇出现在类别标签中，label序列中对应位置标记为class id\n            if word in opt.label2class:\n                label_idx[idx] = opt.label2class[word]\n                # 用[MASK]标记替换标记化器词汇表中没有的标签名\n                if word not in opt.tokenizer.get_vocab():\n                    wordpcs = [opt.tokenizer.mask_token]\n            new_word = ''.join(wordpcs)\n            if new_word != opt.tokenizer.unk_token:\n                idx += len(wordpcs)\n                new_doc.append(new_word)\n            wordpcs = []\n    # 如果找到了匹配的标签，返回替换后的文本和对应位置的索引，否则返回None\n    if (label_idx >= 0).any():\n        return ' '.join(new_doc), label_idx\n    else:\n        return None\n\ndef make_dataloader(data_dict, batch_size):\n    \"\"\"\n    根据已填充的数据集字典，创建并返回DataLoader\n    \"\"\"\n    if \"labels\" in data_dict:\n        dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"], data_dict[\"labels\"])\n    else:\n        dataset = TensorDataset(data_dict[\"input_ids\"], data_dict[\"attention_masks\"])\n    dataset_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    return dataset_loader","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:45:04.477378Z","iopub.execute_input":"2022-04-17T06:45:04.477902Z","iopub.status.idle":"2022-04-17T06:45:04.506164Z","shell.execute_reply.started":"2022-04-17T06:45:04.477868Z","shell.execute_reply":"2022-04-17T06:45:04.505304Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"def create_category_vocabulary(opt, model, label_name_data, loader_name, top_pred_num=50, category_vocab_size=100):\n    \"\"\"\n    构建类别词汇表\n    \"\"\"\n    # 尝试从文件中直接加载分类词汇表\n    loader_file = os.path.join(opt.local_model_dir, loader_name)\n    if os.path.exists(loader_file):\n        print(f\"从 {loader_file} 文件中加载分类词汇表\")\n        category_vocab = torch.load(loader_file)\n    else:\n        print(\"构建分类词汇表\")\n\n        model.eval()\n        # [\"input_ids\",\"attention_masks\",\"labels\"]\n        label_name_dataset_loader = make_dataloader(label_name_data, opt.eval_batch_size)\n        # 统计分类标签的出现频率 {0:{},1:{},2:{},3:{}}\n        category_words_freq = {i: defaultdict(float) for i in range(opt.num_class)}\n        \n        for batch in tqdm(label_name_dataset_loader):\n            with torch.no_grad():\n                input_ids = batch[0].to(opt.device)\n                input_mask = batch[1].to(opt.device)\n                label_pos = batch[2].to(opt.device)\n                match_idx = label_pos >= 0\n                # 进行MLM推理\n                predictions = model(input_ids,\n                                    pred_mode=\"mlm\",\n                                    token_type_ids=None, \n                                    attention_mask=input_mask)\n                # 过滤有分类值的logits，提取每个分类token值最大的前50个索引\n                _, sorted_res = torch.topk(predictions[match_idx], top_pred_num, dim=-1)\n                # 提取出语料中所有的类别值\n                label_idx = label_pos[match_idx]\n                for i, word_list in enumerate(sorted_res):\n                    for j, word_id in enumerate(word_list):\n                        # 分别统计各分类token索引出现的次数\n                        category_words_freq[label_idx[i].item()][word_id.item()] += 1\n        \n        # 过滤掉停用词和属于多分类的词汇后结果存入self.category_vocab\n        # 存储结构 {category_id:[token_id,...],...}\n        category_vocab = filter_keywords(opt, category_words_freq, category_vocab_size)\n\n        # 保存到文件\n        torch.save(category_vocab, loader_file)\n    for i, cat_vocab in category_vocab.items():\n        print(f\"Class {i} category vocabulary: {[opt.inv_vocab[w] for w in cat_vocab]}\\n\")\n    \n    return category_vocab\n\ndef filter_keywords(opt, category_words_freq, category_vocab_size=100):\n    \"\"\"\n    过滤掉停用词和多重分类词\n    \"\"\"\n    # 每个token在语料中的类别列表\n    all_words = defaultdict(list)\n    # 筛选后的分类token字典\n    sorted_dicts = {}\n    # 每个分类中的token计数排序，只保留前100个  {0:{token_id:counts,...},...}\n    for i, cat_dict in category_words_freq.items():\n        sorted_dict = {k:v for k, v in sorted(cat_dict.items(), key=lambda item: item[1], reverse=True)[:category_vocab_size]}\n        sorted_dicts[i] = sorted_dict\n        for word_id in sorted_dict:\n            all_words[word_id].append(i)\n    # 查找在多个分类中出现的token\n    repeat_words = []  \n    for word_id in all_words:\n        if len(all_words[word_id]) > 1:\n            repeat_words.append(word_id)\n    # 提取每个分类中的token_id\n    category_vocab = {}  \n    for i, sorted_dict in sorted_dicts.items(): \n        category_vocab[i] = np.array(list(sorted_dict.keys()))\n    # nltk stopwords\n    stopwords_vocab = stopwords.words('english')  \n    for i, word_list in category_vocab.items():\n        delete_idx = []\n        for j, word_id in enumerate(word_list):\n            word = opt.inv_vocab[word_id]\n            # 不删除分类标签\n            if word in opt.label_name_dict[i]:  \n                continue\n            # 删除不是纯字母的token，长度为1的token，stopword的token，在多个分类中匹配的词汇\n            if not word.isalpha() or len(word) == 1 or word in stopwords_vocab or word_id in repeat_words:\n                delete_idx.append(j)\n        # 删除后的词汇\n        category_vocab[i] = np.delete(category_vocab[i], delete_idx)\n\n    return category_vocab","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:45:04.509465Z","iopub.execute_input":"2022-04-17T06:45:04.509981Z","iopub.status.idle":"2022-04-17T06:45:04.528355Z","shell.execute_reply.started":"2022-04-17T06:45:04.509928Z","shell.execute_reply":"2022-04-17T06:45:04.527589Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"#### masked category prediction(MCP)","metadata":{}},{"cell_type":"code","source":"# mask分类预测\ndef mcp(opt, model, train_data, category_vocab, top_pred_num=50, match_threshold=20):\n    \"\"\"\n    masked 分类预测\n    \"\"\"\n    # 尝试加载mcp分类模型\n    loader_file = os.path.join(opt.local_model_dir, opt.mcp_load_file)\n    if os.path.exists(loader_file):\n        print(f\"\\n从 {loader_file} 文件中加载通过masked进行类别预测训练的模型\")\n    else:\n        # 准备分类模型数据\n        mcp_data = prepare_mcp(opt, model, train_data, category_vocab, top_pred_num, match_threshold)\n        print(f\"\\n通过masked进行类别预测的模型训练\")\n        # mcp训练\n        mcp_train(opt, model, mcp_data)\n    model.load_state_dict(torch.load(loader_file))\n    return model\n\n# masked 分类预测\ndef mcp_train(opt, model, mcp_data):\n    mcp_dataset_loader = make_dataloader(mcp_data, opt.train_batch_size)\n    total_steps = len(mcp_dataset_loader) * opt.mcp_epochs / opt.accum_steps\n    mcp_loss = nn.CrossEntropyLoss()\n    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, eps=1e-8)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n    \n    for i in range(opt.mcp_epochs):\n        model.train()\n        total_train_loss = 0\n        print(f\"Epoch {i+1}:\")\n        model.zero_grad()\n        for j, batch in enumerate(tqdm(mcp_dataset_loader)):\n            input_ids = batch[0].to(opt.device)\n            input_mask = batch[1].to(opt.device)\n            labels = batch[2].to(opt.device)\n            mask_pos = labels >= 0\n            labels = labels[mask_pos]\n            # 屏蔽分类相关的指示词\n            input_ids[mask_pos] = opt.mask_id\n            logits = model(input_ids, \n                            pred_mode=\"classification\",\n                            token_type_ids=None, \n                            attention_mask=input_mask)\n            # 筛选出与lable相关的logits值\n            logits = logits[mask_pos]\n            loss = mcp_loss(logits.view(-1, opt.num_class), labels.view(-1)) / opt.accum_steps\n            total_train_loss += loss.item()\n            loss.backward()\n            if (j+1) % opt.accum_steps == 0:\n                # 梯度裁剪为1.0\n                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n        avg_train_loss = torch.tensor([total_train_loss / len(mcp_dataset_loader) * opt.accum_steps]).to(opt.device)\n        \n        print(f\"Average training loss: {avg_train_loss.mean().item()}\")\n    \n    loader_file = os.path.join(opt.local_model_dir, opt.mcp_load_file)\n    torch.save(model.state_dict(), loader_file)\n    \n# 为自监督的masked分类预测做准备\ndef prepare_mcp(opt, model, train_data, category_vocab, top_pred_num=50, match_threshold=20):\n    # 尝试加载mcp训练数据\n    loader_file = os.path.join(opt.local_model_dir, opt.mcp_train_load_file)\n    if os.path.exists(loader_file):\n        print(f\"Loading masked category prediction data from {loader_file}\")\n        mcp_data = torch.load(loader_file)\n    else:\n        print(\"准备自监督的masked分类预测\")\n        model.eval()\n        # 创建模型用的Dataloader\n        train_dataset_loader = make_dataloader(train_data, opt.eval_batch_size)\n        all_input_ids = []\n        all_mask_label = []\n        all_input_mask = []\n        category_doc_num = defaultdict(int)\n\n        for batch in tqdm(train_dataset_loader):\n            with torch.no_grad():\n                input_ids = batch[0].to(opt.device)\n                input_mask = batch[1].to(opt.device)\n                predictions = model(input_ids,\n                                    pred_mode=\"mlm\",\n                                    token_type_ids=None,\n                                    attention_mask=input_mask)\n                # 提取vocabulary值最大的前50个token\n                _, sorted_res = torch.topk(predictions, top_pred_num, dim=-1)\n                # 遍历分类词汇表\n                for i, cat_vocab in category_vocab.items():\n                    # 默认值为0的匹配索引集合\n                    match_idx = torch.zeros_like(sorted_res).bool()\n                    # 筛选预测和当前类别的分类词汇表的相同项，使用逻辑或运算拼接\n                    for word_id in cat_vocab:\n                        match_idx = (sorted_res == word_id) | match_idx\n                    # 统计匹配的筛选分类词汇数量\n                    match_count = torch.sum(match_idx.int(), dim=-1)\n                    # 至少有20个词汇和分类词汇表相同\n                    valid_idx = (match_count > match_threshold) & (input_mask > 0)\n                    # 筛选满足条件的语句\n                    valid_doc = torch.sum(valid_idx, dim=-1) > 0\n                    # 只要有满足条件的语句,就收集该语句的模型输入张量,并记录对应类别下\n                    if valid_doc.any():\n                        # 准备masked_label矩阵\n                        mask_label = -1 * torch.ones_like(input_ids)\n                        # 满足条件语句中token位置标记分类id\n                        mask_label[valid_idx] = i\n                        # 模型训练的输入数据\n                        all_input_ids.append(input_ids[valid_doc].cpu())\n                        all_mask_label.append(mask_label[valid_doc].cpu())\n                        all_input_mask.append(input_mask[valid_doc].cpu())\n                        # 类别中满足条件的token数量\n                        category_doc_num[i] += valid_doc.int().sum().item()\n        # 拼接list，转换为张量\n        all_input_ids = torch.cat(all_input_ids, dim=0)\n        all_mask_label = torch.cat(all_mask_label, dim=0)\n        all_input_mask = torch.cat(all_input_mask, dim=0)\n        \n        \n        mcp_data = {\n            \"input_ids\": all_input_ids, \n            \"attention_masks\": all_input_mask, \n            \"labels\": all_mask_label,\n            \"category_doc_num\": category_doc_num\n        }\n        # mcp训练数据存盘\n        torch.save(mcp_data, loader_file)\n\n    print(f\"为每个类别找到的带有类别指示性术语的文档数量为: {mcp_data['category_doc_num']}\")\n    for i in mcp_data[\"category_doc_num\"]:\n        assert mcp_data[\"category_doc_num\"][i] > 10, f\"为类别{i}找到的带有类别指示性词汇({category_doc_num[i]})的文档太少;\" \\\n                \"尝试向训练语料库添加更多未标记文档(推荐)或减少`-- match_threshold `参数的值(不推荐)\"\n    print(f\"共有{len(mcp_data['input_ids'])}个文档带有类别指示性词汇。\")\n\n    return mcp_data","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:45:04.531671Z","iopub.execute_input":"2022-04-17T06:45:04.531977Z","iopub.status.idle":"2022-04-17T06:45:04.559727Z","shell.execute_reply.started":"2022-04-17T06:45:04.531929Z","shell.execute_reply":"2022-04-17T06:45:04.558972Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"#### self train","metadata":{}},{"cell_type":"code","source":"def self_train(opt, model, train_data, test_data=None):\n    loader_file = os.path.join(opt.local_model_dir, opt.final_model)\n    if os.path.exists(loader_file):\n        print(f\"\\n发现 {loader_file} 中的最终模型存盘, 跳过self_training\")\n    else:\n        # 生成乱序索引(random permute)\n        rand_idx = torch.randperm(len(train_data[\"input_ids\"]))\n        # 训练数据\n        train_data = {\n            \"input_ids\": train_data[\"input_ids\"][rand_idx],\n            \"attention_masks\": train_data[\"attention_masks\"][rand_idx]\n        }\n        print(f\"\\nStart self-training.\")\n\n        # 如果test_data不为空,就生成测试集data loader\n        test_dataset_loader = make_dataloader(test_data, opt.eval_batch_size) if test_data is not None else None\n        # 训练步数\n        total_steps = int(len(train_data[\"input_ids\"]) * opt.self_train_epochs / (opt.train_batch_size * opt.accum_steps))\n        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6, eps=1e-8)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*total_steps, num_training_steps=total_steps)\n        idx = 0\n        if opt.early_stop:\n            agree_count = 0\n        for i in range(int(total_steps / opt.update_interval)):\n            # 准备自训练的数据\n            self_train_dict, idx, agree = prepare_self_train_data(opt, model, train_data, idx)\n            # 如果当前预测连续3次与更新的目标分布一致，则提前停止训练\n            if opt.early_stop:\n                if 1 - agree < 1e-3:\n                    agree_count += 1\n                else:\n                    agree_count = 0\n                if agree_count >= 3:\n                    break\n            self_train_dataset_loader = make_dataloader(self_train_dict, opt.train_batch_size)\n            self_train_batches(opt, model, self_train_dataset_loader, optimizer, scheduler, test_dataset_loader)\n        \n        loader_file = os.path.join(opt.local_model_dir, opt.final_model)\n        print(f\"保存训练好的模型到 {loader_file} 文件\")\n        torch.save(model.state_dict(), loader_file)\n\ndef prepare_self_train_data(opt, model, train_data, idx):\n    \"\"\"\n    准备自训练的数据和目标\n    \"\"\"\n    # 拆分数据批次大小 batch_size * update_interval * accum_steps\n    target_num = min(opt.train_batch_size * opt.update_interval * opt.accum_steps, len(train_data[\"input_ids\"]))\n    # 生成并修正数据筛选的索引值\n    if idx + target_num >= len(train_data[\"input_ids\"]):\n        select_idx = torch.cat((torch.arange(idx, len(train_data[\"input_ids\"])),\n                                torch.arange(idx + target_num - len(train_data[\"input_ids\"]))))\n    else:\n        select_idx = torch.arange(idx, idx + target_num)\n    assert len(select_idx) == target_num\n    idx = (idx + len(select_idx)) % len(train_data[\"input_ids\"])\n    select_dataset = {\n        \"input_ids\": train_data[\"input_ids\"][select_idx],\n        \"attention_masks\": train_data[\"attention_masks\"][select_idx]\n    }\n\n    dataset_loader = make_dataloader(select_dataset, opt.eval_batch_size)\n    # 模型分类，利用[CLS]的输出进行推理\n    input_ids, input_mask, all_preds = inference(opt, model, dataset_loader, return_type=\"data\")\n\n    # soft labeling\n    weight = all_preds**2 / torch.sum(all_preds, dim=0)\n    target_dist = (weight.t() / torch.sum(weight, dim=1)).t()\n    all_target_pred = target_dist.argmax(dim=-1)\n\n    agree = (all_preds.argmax(dim=-1) == all_target_pred).int().sum().item() / len(all_target_pred)\n\n    self_train_dict = {\n        \"input_ids\": input_ids, \n        \"attention_masks\": input_mask, \n        \"labels\": target_dist\n    }\n    return self_train_dict, idx, agree\n    \ndef self_train_batches(opt, model, self_train_loader, optimizer, scheduler, test_dataset_loader):\n    \"\"\"\n    使用带有目标标签的批数据训练模型\n    \"\"\"\n    st_loss = nn.KLDivLoss(reduction='batchmean')\n    model.train()\n    total_train_loss = 0\n    wrap_train_dataset_loader = tqdm(self_train_loader,desc='self training')\n    model.zero_grad()\n    \n    for j, batch in enumerate(wrap_train_dataset_loader):\n        input_ids = batch[0].to(opt.device)\n        input_mask = batch[1].to(opt.device)\n        target_dist = batch[2].to(opt.device)\n        logits = model(input_ids,\n                        pred_mode=\"classification\",\n                        token_type_ids=None,\n                        attention_mask=input_mask)\n        logits = logits[:, 0, :]\n        # 先算softmax再计算log\n        preds = nn.LogSoftmax(dim=-1)(logits)\n        loss = st_loss(preds.view(-1, opt.num_class), target_dist.view(-1, opt.num_class)) / opt.accum_steps\n        total_train_loss += loss.item()\n        loss.backward()\n        if (j+1) % opt.accum_steps == 0:\n            # Clip the norm of the gradients to 1.0.\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n    if test_dataset_loader is not None:\n        acc = inference(opt, model, test_dataset_loader, return_type=\"acc\")\n        acc = torch.tensor(acc).mean().item()\n\n    avg_train_loss = torch.tensor([total_train_loss / len(wrap_train_dataset_loader) * opt.accum_steps]).to(opt.device)\n\n    print(f\"lr: {optimizer.param_groups[0]['lr']:.4g}\")\n    print(f\"Average training loss: {avg_train_loss.mean().item()}\")\n    if test_dataset_loader is not None:\n        print(f\"Test acc: {acc}\")\n        \ndef inference(opt, model, dataset_loader, return_type):\n    \"\"\"\n    使用模型对Dataloader进行推理,针对return_type参数不同取值,返回不同结果\n    \"\"\"\n    if return_type == \"data\":\n        all_input_ids = []\n        all_input_mask = []\n        all_preds = []\n    elif return_type == \"acc\":\n        pred_labels = []\n        truth_labels = []\n    elif return_type == \"pred\":\n        pred_labels = []\n    model.eval()\n    \n    for batch in tqdm(dataset_loader,desc=f'{return_type} processing'):\n        with torch.no_grad():\n            input_ids = batch[0].to(opt.device)\n            input_mask = batch[1].to(opt.device)\n            logits = model(input_ids,\n                            pred_mode=\"classification\",\n                            token_type_ids=None,\n                            attention_mask=input_mask)\n            # 提取[CLS]的logits进行文本分类推理\n            logits = logits[:,0,:]\n            if return_type == \"data\":\n                all_input_ids.append(input_ids)\n                all_input_mask.append(input_mask)\n                all_preds.append(nn.Softmax(dim=-1)(logits))\n            elif return_type == \"acc\":\n                labels = batch[2]\n                pred_labels.append(torch.argmax(logits, dim=-1).cpu())\n                truth_labels.append(labels)\n            elif return_type == \"pred\":\n                pred_labels.append(torch.argmax(logits, dim=-1).cpu())\n\n    if return_type == \"data\":\n        all_input_ids = torch.cat(all_input_ids, dim=0)\n        all_input_mask = torch.cat(all_input_mask, dim=0)\n        all_preds = torch.cat(all_preds, dim=0)\n        return all_input_ids, all_input_mask, all_preds\n    elif return_type == \"acc\":\n        pred_labels = torch.cat(pred_labels, dim=0)\n        truth_labels = torch.cat(truth_labels, dim=0)\n        samples = len(truth_labels)\n        acc = (pred_labels == truth_labels).float().sum() / samples\n        return acc.to(opt.device)\n    elif return_type == \"pred\":\n        pred_labels = torch.cat(pred_labels, dim=0)\n        return pred_labels","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:45:04.561829Z","iopub.execute_input":"2022-04-17T06:45:04.562371Z","iopub.status.idle":"2022-04-17T06:45:04.597667Z","shell.execute_reply.started":"2022-04-17T06:45:04.562297Z","shell.execute_reply":"2022-04-17T06:45:04.596991Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"#### 模型训练","metadata":{}},{"cell_type":"code","source":"# 创建模型对象\nmodel = LOTClassModel.from_pretrained(\n    opt.bert_model,\n    output_attentions=False,\n    output_hidden_states=False,\n    num_labels=opt.num_class\n)\nmodel.to(opt.device)\n\n# 构建分类别词汇表\nlabel_name_data = create_label_name_dataset(opt, opt.train_file, opt.label_name_load_file)\ncategory_vocab = create_category_vocabulary(\n    opt = opt, \n    model = model, \n    label_name_data = label_name_data, \n    loader_name = opt.category_vocab_load_file, \n    top_pred_num=opt.top_pred_num, \n    category_vocab_size=opt.category_vocab_size\n)\n\n# 训练mask类别的预测\ninput_data = create_input_dataset(opt, opt.train_file, opt.train_load_file)\nmcp_model = mcp(\n    opt=opt, \n    model=model, \n    train_data=input_data, \n    category_vocab = category_vocab,\n    top_pred_num=opt.top_pred_num, \n    match_threshold=opt.match_threshold\n)\n\n# 自训练\ntest_data = create_input_dataset(opt, opt.test_file, opt.test_load_file, opt.test_label_file)\nself_train(opt, mcp_model, input_data, test_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T06:45:04.598839Z","iopub.execute_input":"2022-04-17T06:45:04.599453Z","iopub.status.idle":"2022-04-17T07:44:57.908701Z","shell.execute_reply.started":"2022-04-17T06:45:04.599414Z","shell.execute_reply":"2022-04-17T07:44:57.907454Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"\nloader_file = os.path.join(opt.local_model_dir, opt.final_model)\nprint(f\"保存训练好的模型到 {loader_file} 文件\")\ntorch.save(model.state_dict(), loader_file)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T07:49:13.738778Z","iopub.execute_input":"2022-04-17T07:49:13.739035Z","iopub.status.idle":"2022-04-17T07:49:14.531921Z","shell.execute_reply.started":"2022-04-17T07:49:13.739007Z","shell.execute_reply":"2022-04-17T07:49:14.531217Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}